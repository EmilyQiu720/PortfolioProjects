{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "# import json\n",
    "# from datetime import datetime\n",
    "# from datetime import datetime, UTC\n",
    "\n",
    "# # Path to your JSONL file\n",
    "# category = 'Digital_Music'\n",
    "\n",
    "# # source files\n",
    "# user_reviews_jsonl_file = f\"Amazon Raw Data/{category}.jsonl\"\n",
    "# item_metadata_jsonl_file = f\"Amazon Raw Data/meta_{category}.jsonl\"\n",
    "\n",
    "# # target files\n",
    "# user_reviews_raw_csv = f\"Amazon Raw Data/{category}_user_reviews_raw.csv\"\n",
    "# item_metadata_raw_csv = f\"Amazon Raw Data/{category}_item_metadata_raw.csv\"\n",
    "# user_reviews_target_csv = f\"Amazon Processed Dataset/{category}_user_reviews.csv\"\n",
    "# item_metadata_target_csv = f\"Amazon Processed Dataset/{category}_item_metadata.csv\"\n",
    "\n",
    "\n",
    "# # Read JSONL file\n",
    "# data = []\n",
    "# with open(user_reviews_jsonl_file, 'r', encoding='utf-8') as f:\n",
    "#     for line in f:\n",
    "#         data.append(json.loads(line.strip()))  # Read each line as a JSON object\n",
    "# user_reviews = pd.DataFrame(data)\n",
    "\n",
    "# # Read JSONL file\n",
    "# data = []\n",
    "# with open(item_metadata_jsonl_file, 'r', encoding='utf-8') as f:\n",
    "#     for line in f:\n",
    "#         data.append(json.loads(line.strip()))  # Read each line as a JSON object\n",
    "# item_metadata = pd.DataFrame(data)\n",
    "\n",
    "# # Perform operations (modify as needed)\n",
    "# # Example: Removing duplicates\n",
    "# user_reviews = user_reviews.astype(str).drop_duplicates()\n",
    "# user_reviews = user_reviews[['user_id', 'parent_asin', 'rating', 'title', 'text', 'timestamp', 'helpful_vote', 'verified_purchase']]\n",
    "# user_reviews['title'] = user_reviews['title'].replace('N9ne', 'None')\n",
    "# user_reviews['title'] = user_reviews['title'].replace('None', '')\n",
    "# user_reviews['text'] = user_reviews['text'].replace('N9ne', 'None')\n",
    "# user_reviews['text'] = user_reviews['text'].replace('None', '')\n",
    "# user_reviews['timestamp'] = pd.to_numeric(user_reviews['timestamp'], errors='coerce')\n",
    "# user_reviews['timestamp'] = pd.to_datetime(user_reviews['timestamp'], errors='coerce', unit='ms').dt.tz_localize(UTC)\n",
    "\n",
    "# # item_metadata = item_metadata[['parent_asin', 'title', 'average_rating', 'rating_number', 'main_category', 'features', 'description', 'price', 'store']]\n",
    "# item_metadata['average_rating'] = item_metadata['average_rating'].fillna(0)\n",
    "# item_metadata['average_rating'] = item_metadata['average_rating'].astype(float)\n",
    "\n",
    "# user_reviews.to_csv(user_reviews_raw_csv, index = False)\n",
    "# item_metadata.to_csv(item_metadata_raw_csv, index = False)\n",
    "\n",
    "# print(f'Number of products = {item_metadata.shape[0]}\\nNumber of reviews = {user_reviews.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of products = 30000\n",
      "Number of reviews = 988594\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import json, random\n",
    "from datetime import datetime\n",
    "from datetime import datetime, UTC\n",
    "\n",
    "# Path to your JSONL file\n",
    "category = 'Video_Games'\n",
    "\n",
    "# Source files\n",
    "user_reviews_jsonl_file = f\"Amazon Raw Data/{category}.jsonl\"\n",
    "item_metadata_jsonl_file = f\"Amazon Raw Data/meta_{category}.jsonl\"\n",
    "\n",
    "# Target files\n",
    "user_reviews_raw_csv = f\"Amazon Raw Data/{category}_user_reviews_raw.csv\"\n",
    "item_metadata_raw_csv = f\"Amazon Raw Data/{category}_item_metadata_raw.csv\"\n",
    "\n",
    "num_samples = 30000  # Number of records to read\n",
    "\n",
    "# Initialize reservoir\n",
    "sampled_records = []\n",
    "with open(item_metadata_jsonl_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    for i, line in enumerate(f):\n",
    "        record = json.loads(line.strip())\n",
    "\n",
    "        # Reservoir sampling logic\n",
    "        if len(sampled_records) < num_samples:\n",
    "            sampled_records.append(record)\n",
    "        else:\n",
    "            rand_idx = random.randint(0, i)\n",
    "            if rand_idx < num_samples:\n",
    "                sampled_records[rand_idx] = record\n",
    "\n",
    "# Convert to DataFrame\n",
    "item_metadata = pd.DataFrame(sampled_records)\n",
    "\n",
    "# ‚úÖ Step 1: Load the 30,000 sampled ASINs into a set for fast lookup\n",
    "selected_asins = set(item_metadata[\"parent_asin\"])\n",
    "\n",
    "# ‚úÖ Step 2: Stream through the user reviews file and extract matching records\n",
    "matched_reviews = []\n",
    "\n",
    "with open(user_reviews_jsonl_file, \"r\", encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        review = json.loads(line.strip())\n",
    "\n",
    "        # ‚úÖ Keep only reviews where `parent_asin` is in the selected ASINs\n",
    "        if review[\"parent_asin\"] in selected_asins:\n",
    "            matched_reviews.append(review)\n",
    "\n",
    "user_reviews = pd.DataFrame(matched_reviews)\n",
    "user_reviews = user_reviews.astype(str).drop_duplicates()\n",
    "user_reviews = user_reviews[['user_id', 'parent_asin', 'rating', 'title', 'text', 'timestamp', 'helpful_vote', 'verified_purchase']]\n",
    "user_reviews['timestamp'] = pd.to_numeric(user_reviews['timestamp'], errors='coerce')\n",
    "user_reviews['timestamp'] = pd.to_datetime(user_reviews['timestamp'], errors='coerce', unit='ms').dt.tz_localize(UTC)\n",
    "user_reviews['title'] = user_reviews['title'].replace('N9ne', 'None')\n",
    "user_reviews['title'] = user_reviews['title'].replace('None', '')\n",
    "user_reviews['text'] = user_reviews['text'].replace('N9ne', 'None')\n",
    "user_reviews['text'] = user_reviews['text'].replace('None', '')\n",
    "\n",
    "# item_metadata = item_metadata[['parent_asin', 'title', 'average_rating', 'rating_number', 'main_category', 'features', 'description', 'price', 'store']]\n",
    "item_metadata['average_rating'] = item_metadata['average_rating'].fillna(0)\n",
    "item_metadata['average_rating'] = item_metadata['average_rating'].astype(float)\n",
    "\n",
    "user_reviews.to_csv(user_reviews_raw_csv, index = False)\n",
    "item_metadata.to_csv(item_metadata_raw_csv, index = False)\n",
    "\n",
    "print(f'Number of products = {item_metadata.shape[0]}\\nNumber of reviews = {user_reviews.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 30000 entries, 0 to 29999\n",
      "Data columns (total 16 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   main_category    27612 non-null  object \n",
      " 1   title            30000 non-null  object \n",
      " 2   average_rating   30000 non-null  float64\n",
      " 3   rating_number    30000 non-null  int64  \n",
      " 4   features         30000 non-null  object \n",
      " 5   description      30000 non-null  object \n",
      " 6   price            13501 non-null  object \n",
      " 7   images           30000 non-null  object \n",
      " 8   videos           30000 non-null  object \n",
      " 9   store            28942 non-null  object \n",
      " 10  categories       30000 non-null  object \n",
      " 11  details          30000 non-null  object \n",
      " 12  parent_asin      30000 non-null  object \n",
      " 13  bought_together  0 non-null      object \n",
      " 14  subtitle         76 non-null     object \n",
      " 15  author           61 non-null     object \n",
      "dtypes: float64(1), int64(1), object(14)\n",
      "memory usage: 3.7+ MB\n"
     ]
    }
   ],
   "source": [
    "item_metadata.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 328,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 988594 entries, 0 to 1000537\n",
      "Data columns (total 8 columns):\n",
      " #   Column             Non-Null Count   Dtype              \n",
      "---  ------             --------------   -----              \n",
      " 0   user_id            988594 non-null  object             \n",
      " 1   parent_asin        988594 non-null  object             \n",
      " 2   rating             988594 non-null  object             \n",
      " 3   title              988594 non-null  object             \n",
      " 4   text               988594 non-null  object             \n",
      " 5   timestamp          988594 non-null  datetime64[ns, UTC]\n",
      " 6   helpful_vote       988594 non-null  object             \n",
      " 7   verified_purchase  988594 non-null  object             \n",
      "dtypes: datetime64[ns, UTC](1), object(7)\n",
      "memory usage: 67.9+ MB\n"
     ]
    }
   ],
   "source": [
    "user_reviews.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Which reviews should we keep?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 329,
   "metadata": {},
   "outputs": [],
   "source": [
    "review_time = user_reviews[['parent_asin', 'timestamp']]\n",
    "review_time = user_reviews.groupby('parent_asin', as_index=False).agg(\n",
    "    no_of_reviews=('timestamp', 'count'),\n",
    "    earliest_timestamp=('timestamp', 'min')\n",
    ")\n",
    "review_time['earliest_year_of_review'] = review_time['earliest_timestamp'].apply(lambda x: x.year)\n",
    "end_timestamp = user_reviews['timestamp'].max()\n",
    "review_time['review_age_mths'] = review_time['earliest_timestamp'].apply(lambda x: (end_timestamp - x).days/30)\n",
    "retain_products = set(review_time[review_time['review_age_mths'] <= 3].index)\n",
    "retain_products = retain_products.union(set(item_metadata[item_metadata['average_rating'] >= 4.5].index))\n",
    "item_metadata = item_metadata.loc[list(retain_products)]\n",
    "item_metadata = pd.merge(left = item_metadata, right = review_time[['parent_asin', 'no_of_reviews', 'review_age_mths']], on = 'parent_asin', how = 'left')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 330,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of products = 8913\n",
      "Number of reviews = 433962\n"
     ]
    }
   ],
   "source": [
    "unique_items = pd.Series(item_metadata['parent_asin'].unique())\n",
    "unique_items.name = 'parent_asin'\n",
    "user_reviews = pd.merge(left = unique_items, right = user_reviews, on = 'parent_asin', how = 'inner')\n",
    "\n",
    "print(f'Number of products = {item_metadata.shape[0]}\\nNumber of reviews = {user_reviews.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 331,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package words to\n",
      "[nltk_data]     C:\\Users\\anira\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package words is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\anira\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import re, emoji, inflect\n",
    "import nltk\n",
    "nltk.download('words')\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords, words\n",
    "\n",
    "slang_dict = {\n",
    "    \"gonna\": \"going to\", \"wanna\": \"want to\", \"idk\": \"i do not know\",\n",
    "    \"b4\": \"before\", \"gr8\": \"great\", \"luv\": \"love\", \"thx\": \"thanks\"\n",
    "}\n",
    "\n",
    "shortened_dict = {\n",
    "    \"lite\": \"light\", \"nite\": \"night\", \"coz\": \"because\", \n",
    "    \"fav\": \"favorite\", \"biz\": \"business\", \"tmrw\": \"tomorrow\", \"u\": \"you\"\n",
    "}\n",
    "\n",
    "# stop_words = set(stopwords.words('english'))\n",
    "# negations = {\"not\", \"no\", \"never\", \"none\", \"nor\", \"n't\"}  # Keep important negations\n",
    "# filtered_stopwords = stop_words - negations\n",
    "word_list = set(words.words()) # Load English word dictionary\n",
    "\n",
    "def reduce_elongation(text):\n",
    "    def replace_match(match):\n",
    "        word = match.group(0)\n",
    "        # Only reduce if it's not a real English word\n",
    "        if word.lower() not in word_list:\n",
    "            return re.sub(r'(.)\\1{2,}', r'\\1\\1', word)  # Keep max 2 repetitions\n",
    "        return word\n",
    "\n",
    "    return re.sub(r'\\b(\\w+?)\\1+\\b', replace_match, text)\n",
    "\n",
    "def replace_number(match):\n",
    "    p = inflect.engine()\n",
    "    num = match.group()\n",
    "    try:\n",
    "        # Ensure input is a valid number\n",
    "        if isinstance(num, (int, float)):  \n",
    "            num = int(num)  # Convert float to int to avoid issues\n",
    "            return p.number_to_words(num)\n",
    "        return ''  # Skip invalid values\n",
    "    except:\n",
    "        return ''  # Skip errors\n",
    "\n",
    "def clean_text(text):\n",
    "    text = str(text).lower()  # Lowercase\n",
    "    text = emoji.demojize(text)  # Convert emojis\n",
    "    text = re.sub(r'<.*?>', ' ', text)  # Remove HTML tags\n",
    "    text = re.sub(r'[^a-z0-9\\s]', ' ', text)  # Remove special characters\n",
    "    text = \" \".join([slang_dict.get(word, word) for word in text.split()])  # Replace slang\n",
    "    text = \" \".join([shortened_dict.get(word, word) for word in text.split()])  # Replace shortened words\n",
    "    text = reduce_elongation(text)  # Remove letter elongations\n",
    "    text = re.sub(r'\\b\\d+\\b', replace_number, text)  # Replace numbers with text\n",
    "    # text = \" \".join([word for word in text.split() if word not in stop_words])  # Remove stopwords\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()  # Remove extra spaces\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_reviews = user_reviews.copy(deep = True)\n",
    "\n",
    "filtered_reviews['review'] = filtered_reviews['title'] + ' ' + filtered_reviews['text']\n",
    "filtered_reviews.drop(['text', 'title'], axis = 1, inplace = True)\n",
    "filtered_reviews['cleaned_review'] = filtered_reviews['review'].apply(clean_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_reviews['review_length'] = filtered_reviews['cleaned_review'].apply(lambda x: len(x.split()))\n",
    "filtered_reviews.drop(filtered_reviews[filtered_reviews['review_length'] <= 5].index, axis = 0, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8447"
      ]
     },
     "execution_count": 334,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(filtered_reviews['parent_asin'].unique())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eliminate reviews for products that do not have reviews after 2013"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anira\\AppData\\Local\\Temp\\ipykernel_30220\\4237903152.py:1: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  res = filtered_reviews.groupby(['parent_asin'], as_index=False).apply(lambda x: max(x['timestamp']))\n"
     ]
    }
   ],
   "source": [
    "res = filtered_reviews.groupby(['parent_asin'], as_index=False).apply(lambda x: max(x['timestamp']))\n",
    "res.columns = ['parent_asin', 'latest_timestamp']\n",
    "res['year'] = res['latest_timestamp'].apply(lambda x: x.year)\n",
    "res.drop(res[res['year'] <= 2013].index, axis = 0, inplace = True)\n",
    "filtered_reviews = pd.merge(left = filtered_reviews, right = res['parent_asin'], on = 'parent_asin', how = 'inner')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 336,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>parent_asin</th>\n",
       "      <th>main_category</th>\n",
       "      <th>title</th>\n",
       "      <th>average_rating</th>\n",
       "      <th>rating_number</th>\n",
       "      <th>features</th>\n",
       "      <th>description</th>\n",
       "      <th>price</th>\n",
       "      <th>images</th>\n",
       "      <th>videos</th>\n",
       "      <th>store</th>\n",
       "      <th>categories</th>\n",
       "      <th>details</th>\n",
       "      <th>bought_together</th>\n",
       "      <th>subtitle</th>\n",
       "      <th>author</th>\n",
       "      <th>no_of_reviews</th>\n",
       "      <th>review_age_mths</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>B085B1Y5YY</td>\n",
       "      <td>All Electronics</td>\n",
       "      <td>KH KAM HUA PSP-S110 Replacement Battery for So...</td>\n",
       "      <td>3.9</td>\n",
       "      <td>11</td>\n",
       "      <td>[PSP-S110 Replacement Battery For SONY PSP-200...</td>\n",
       "      <td>[- Compatible Model:, SONY Lite, PSP 2th, PSP-...</td>\n",
       "      <td>10.99</td>\n",
       "      <td>[{'thumb': 'https://m.media-amazon.com/images/...</td>\n",
       "      <td>[{'title': 'OSTENT Real 1400mAh 3.6V Lithium I...</td>\n",
       "      <td>KH KAM HUA</td>\n",
       "      <td>[Video Games, Legacy Systems, PlayStation Syst...</td>\n",
       "      <td>{'Package Dimensions': '3.23 x 2.2 x 0.71 inch...</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>5</td>\n",
       "      <td>40.200000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B07SZJZV88</td>\n",
       "      <td>Video Games</td>\n",
       "      <td>Nintendo Selects: The Legend of Zelda Ocarina ...</td>\n",
       "      <td>4.9</td>\n",
       "      <td>22</td>\n",
       "      <td>[Authentic Nintendo Selects: The Legend of Zel...</td>\n",
       "      <td>[]</td>\n",
       "      <td>37.42</td>\n",
       "      <td>[{'thumb': 'https://m.media-amazon.com/images/...</td>\n",
       "      <td>[]</td>\n",
       "      <td>Amazon Renewed</td>\n",
       "      <td>[Video Games, Legacy Systems, Nintendo Systems...</td>\n",
       "      <td>{'Best Sellers Rank': {'Video Games': 51019, '...</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>3</td>\n",
       "      <td>46.466667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>B00GZ5222I</td>\n",
       "      <td>Video Games</td>\n",
       "      <td>HEX Blue - Decal Style Skin fits original PS4 ...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>[Skin covers top, front and side of the consol...</td>\n",
       "      <td>[WraptorSkinzTM are self adhering vinyl skins ...</td>\n",
       "      <td>16.95</td>\n",
       "      <td>[{'thumb': 'https://m.media-amazon.com/images/...</td>\n",
       "      <td>[]</td>\n",
       "      <td>WraptorSkinz</td>\n",
       "      <td>[Video Games, PlayStation 4, Accessories, Face...</td>\n",
       "      <td>{'Pricing': 'The strikethrough price is the Li...</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>2</td>\n",
       "      <td>117.266667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>B078TRY1C5</td>\n",
       "      <td>Video Games</td>\n",
       "      <td>Monster Hunter Official Research Commission Ke...</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2</td>\n",
       "      <td>[]</td>\n",
       "      <td>[]</td>\n",
       "      <td>None</td>\n",
       "      <td>[{'thumb': 'https://m.media-amazon.com/images/...</td>\n",
       "      <td>[]</td>\n",
       "      <td>Numskull</td>\n",
       "      <td>[]</td>\n",
       "      <td>{'Pricing': 'The strikethrough price is the Li...</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>65.666667</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>B0BKL4RH8B</td>\n",
       "      <td>None</td>\n",
       "      <td>yuailiur 500 Pockets Game Cards Binder,card Ho...</td>\n",
       "      <td>4.6</td>\n",
       "      <td>10</td>\n",
       "      <td>[üéÆüéÆ„ÄêExquisite Craftsmanship &amp; Premium Material...</td>\n",
       "      <td>[]</td>\n",
       "      <td>16.99</td>\n",
       "      <td>[{'thumb': 'https://m.media-amazon.com/images/...</td>\n",
       "      <td>[]</td>\n",
       "      <td>yuailiur</td>\n",
       "      <td>[Video Games, Nintendo Switch, Accessories, Ca...</td>\n",
       "      <td>{'Package Dimensions': '9.76 x 7.36 x 2.13 inc...</td>\n",
       "      <td>None</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>1</td>\n",
       "      <td>7.600000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  parent_asin    main_category  \\\n",
       "0  B085B1Y5YY  All Electronics   \n",
       "1  B07SZJZV88      Video Games   \n",
       "2  B00GZ5222I      Video Games   \n",
       "3  B078TRY1C5      Video Games   \n",
       "4  B0BKL4RH8B             None   \n",
       "\n",
       "                                               title  average_rating  \\\n",
       "0  KH KAM HUA PSP-S110 Replacement Battery for So...             3.9   \n",
       "1  Nintendo Selects: The Legend of Zelda Ocarina ...             4.9   \n",
       "2  HEX Blue - Decal Style Skin fits original PS4 ...             5.0   \n",
       "3  Monster Hunter Official Research Commission Ke...             5.0   \n",
       "4  yuailiur 500 Pockets Game Cards Binder,card Ho...             4.6   \n",
       "\n",
       "   rating_number                                           features  \\\n",
       "0             11  [PSP-S110 Replacement Battery For SONY PSP-200...   \n",
       "1             22  [Authentic Nintendo Selects: The Legend of Zel...   \n",
       "2              2  [Skin covers top, front and side of the consol...   \n",
       "3              2                                                 []   \n",
       "4             10  [üéÆüéÆ„ÄêExquisite Craftsmanship & Premium Material...   \n",
       "\n",
       "                                         description  price  \\\n",
       "0  [- Compatible Model:, SONY Lite, PSP 2th, PSP-...  10.99   \n",
       "1                                                 []  37.42   \n",
       "2  [WraptorSkinzTM are self adhering vinyl skins ...  16.95   \n",
       "3                                                 []   None   \n",
       "4                                                 []  16.99   \n",
       "\n",
       "                                              images  \\\n",
       "0  [{'thumb': 'https://m.media-amazon.com/images/...   \n",
       "1  [{'thumb': 'https://m.media-amazon.com/images/...   \n",
       "2  [{'thumb': 'https://m.media-amazon.com/images/...   \n",
       "3  [{'thumb': 'https://m.media-amazon.com/images/...   \n",
       "4  [{'thumb': 'https://m.media-amazon.com/images/...   \n",
       "\n",
       "                                              videos           store  \\\n",
       "0  [{'title': 'OSTENT Real 1400mAh 3.6V Lithium I...      KH KAM HUA   \n",
       "1                                                 []  Amazon Renewed   \n",
       "2                                                 []    WraptorSkinz   \n",
       "3                                                 []        Numskull   \n",
       "4                                                 []        yuailiur   \n",
       "\n",
       "                                          categories  \\\n",
       "0  [Video Games, Legacy Systems, PlayStation Syst...   \n",
       "1  [Video Games, Legacy Systems, Nintendo Systems...   \n",
       "2  [Video Games, PlayStation 4, Accessories, Face...   \n",
       "3                                                 []   \n",
       "4  [Video Games, Nintendo Switch, Accessories, Ca...   \n",
       "\n",
       "                                             details bought_together subtitle  \\\n",
       "0  {'Package Dimensions': '3.23 x 2.2 x 0.71 inch...            None      NaN   \n",
       "1  {'Best Sellers Rank': {'Video Games': 51019, '...            None      NaN   \n",
       "2  {'Pricing': 'The strikethrough price is the Li...            None      NaN   \n",
       "3  {'Pricing': 'The strikethrough price is the Li...            None      NaN   \n",
       "4  {'Package Dimensions': '9.76 x 7.36 x 2.13 inc...            None      NaN   \n",
       "\n",
       "  author  no_of_reviews  review_age_mths  \n",
       "0    NaN              5        40.200000  \n",
       "1    NaN              3        46.466667  \n",
       "2    NaN              2       117.266667  \n",
       "3    NaN              1        65.666667  \n",
       "4    NaN              1         7.600000  "
      ]
     },
     "execution_count": 336,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "unique_items = pd.Series(filtered_reviews['parent_asin'].unique())\n",
    "unique_items.name = 'parent_asin'\n",
    "filtered_items = pd.merge(left = unique_items, right = item_metadata, on = 'parent_asin', how = 'left')\n",
    "filtered_items.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of products = 7718\n",
      "Number of reviews = 383211\n"
     ]
    }
   ],
   "source": [
    "print(f'Number of products = {filtered_items.shape[0]}\\nNumber of reviews = {filtered_reviews.shape[0]}')\n",
    "# filtered_reviews.to_csv(user_reviews_target_csv, index = False)\n",
    "# filtered_items.to_csv(item_metadata_target_csv, index = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load Filtered Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 338,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import numpy as np\n",
    "# import pandas as pd\n",
    "# user_reviews_target_csv = f\"Amazon Processed Dataset/{category}_user_reviews.csv\"\n",
    "# item_metadata_target_csv = f\"Amazon Processed Dataset/{category}_item_metadata.csv\"\n",
    "# user_reviews = pd.read_csv(user_reviews_target_csv, parse_dates=['timestamp'])\n",
    "# item_metadata = pd.read_csv(item_metadata_target_csv)\n",
    "user_reviews = filtered_reviews\n",
    "item_metadata = filtered_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_reviews['rating'] = user_reviews['rating'].astype(float).astype(int)\n",
    "user_reviews['helpful_vote'] = user_reviews['helpful_vote'].astype(int).fillna(0)\n",
    "user_reviews['verified_purchase'] = user_reviews['verified_purchase'].astype(bool).astype(int)\n",
    "\n",
    "item_metadata['rating_number'] = item_metadata['rating_number'].fillna(0)\n",
    "item_metadata['rating_number'] = item_metadata['rating_number'].astype(int)\n",
    "item_metadata['main_category'] = item_metadata['main_category'].fillna('Unknown')\n",
    "item_metadata['main_category'] = item_metadata['main_category'].replace('None', 'Unknown')\n",
    "item_metadata['price'] = item_metadata['price'].replace('None', 0.0)\n",
    "item_metadata['price'] = pd.to_numeric(item_metadata['price'], errors='coerce')\n",
    "item_metadata['store'] = item_metadata['store'].fillna('Unknown')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Eliminate short term products "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anira\\AppData\\Local\\Temp\\ipykernel_30220\\1590593140.py:3: UserWarning: This pattern is interpreted as a regular expression, and has match groups. To actually get the groups, use str.extract.\n",
      "  item_metadata['short_term_product'] = item_metadata['title'].str.contains(pattern, case=False, na=False, regex=True)\n"
     ]
    }
   ],
   "source": [
    "product_exclusion_keywords = ['clearance', 'wholesale', 'discount', 'final sale', 'trial']\n",
    "pattern = r'\\b(' + '|'.join(product_exclusion_keywords) + r')\\b'\n",
    "item_metadata['short_term_product'] = item_metadata['title'].str.contains(pattern, case=False, na=False, regex=True)\n",
    "item_metadata.drop(item_metadata[item_metadata['short_term_product']].index, axis = 0, inplace = True)\n",
    "item_metadata.drop(['short_term_product'], axis = 1, inplace  = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert array to text\n",
    "item_metadata.drop(item_metadata[item_metadata['title'].isna()].index, axis = 0, inplace = True)\n",
    "item_metadata['features'] = item_metadata['features'].apply(lambda x: ' '.join(map(str, x)) if isinstance(x, list) else str(x))\n",
    "item_metadata['description'] = item_metadata['description'].apply(lambda x: ' '.join(map(str, x)) if isinstance(x, list) else str(x))\n",
    "\n",
    "category_str = ' '.join(category.split('_')) if len(user_reviews.columns.intersection(['categories'])) == 0 else item_metadata['categories']\n",
    "item_metadata['context'] = item_metadata['title'] + ' ' + item_metadata['features'] + ' ' + item_metadata['description'] + ' ' + category_str + ' ' + item_metadata['main_category']\n",
    "item_metadata['cleaned_context'] = item_metadata['context'].apply(clean_text)\n",
    "item_metadata.drop(item_metadata[item_metadata['cleaned_context'].isna()].index, axis = 0, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Remove corresponding reviews"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 342,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of products = 7714\n",
      "Number of reviews = 383198\n"
     ]
    }
   ],
   "source": [
    "unique_items = pd.Series(item_metadata['parent_asin'].unique())\n",
    "unique_items.name = 'parent_asin'\n",
    "user_reviews = pd.merge(left = unique_items, right = user_reviews, on = 'parent_asin', how = 'inner')\n",
    "\n",
    "print(f'Number of products = {item_metadata.shape[0]}\\nNumber of reviews = {user_reviews.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 343,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import linregress\n",
    "\n",
    "# reg_reviews = user_reviews.copy(deep = True)\n",
    "\n",
    "def compute_rating_slope(df):\n",
    "    slopes = {}\n",
    "    \n",
    "    for product, group in df.groupby('parent_asin'):\n",
    "        if len(group) > 1:  # Need at least 2 points to compute a trend\n",
    "            group = group.sort_values(by='timestamp')\n",
    "            days_since_start = (group['timestamp'] - group['timestamp'].min()).dt.total_seconds() / (24 * 3600)\n",
    "\n",
    "            # Aggregate ratings for the same day to prevent duplicates\n",
    "            daily_avg_ratings = group.groupby(days_since_start).agg({'rating': 'mean'}).reset_index()\n",
    "\n",
    "            if daily_avg_ratings.index.nunique() == 1:\n",
    "                slopes[product] = 0 \n",
    "            else:\n",
    "                slope, _, _, _, _ = linregress(daily_avg_ratings['timestamp'], daily_avg_ratings['rating'])\n",
    "                slopes[product] = slope\n",
    "        else:\n",
    "            slopes[product] = 0\n",
    "    \n",
    "    return slopes\n",
    "\n",
    "# Compute slopes\n",
    "slopes_dict = compute_rating_slope(user_reviews)\n",
    "item_metadata['rating_trend_slope'] = item_metadata['parent_asin'].map(slopes_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### User Review Compilation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\anira\\AppData\\Local\\Temp\\ipykernel_30220\\1662124481.py:23: DeprecationWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  df_product_sentiment = user_reviews.groupby('parent_asin').apply(\n"
     ]
    }
   ],
   "source": [
    "from vaderSentiment.vaderSentiment import SentimentIntensityAnalyzer\n",
    "import numpy as np\n",
    "\n",
    "analyzer = SentimentIntensityAnalyzer()\n",
    "user_reviews['sentiment'] = user_reviews['cleaned_review'].apply(lambda x: analyzer.polarity_scores(x)['compound'])\n",
    "\n",
    "# Compute latest review date per product\n",
    "user_reviews['latest_review_date'] = user_reviews.groupby('parent_asin')['timestamp'].transform('max')\n",
    "\n",
    "# Compute relative days since each review (relative to latest review per product)\n",
    "user_reviews['days_since_review'] = (user_reviews['latest_review_date'] - user_reviews['timestamp']).dt.days\n",
    "\n",
    "# Set decay factor (Œª) - adjust this value as needed\n",
    "lambda_factor = 0.01  # Higher value means faster decay\n",
    "\n",
    "# Compute time decay weights (exponential decay function)\n",
    "user_reviews['time_weight'] = np.exp(-lambda_factor * user_reviews['days_since_review'])\n",
    "\n",
    "# Normalize weights per product (so they sum to 1)\n",
    "user_reviews['time_weight'] /= user_reviews.groupby('parent_asin')['time_weight'].transform('sum')\n",
    "\n",
    "# Compute weighted sentiment per product, keeping [-1,1] range\n",
    "df_product_sentiment = user_reviews.groupby('parent_asin').apply(\n",
    "    lambda x: np.average(x['sentiment'], weights=x['time_weight'])\n",
    ").reset_index(name='time_weighted_sentiment')\n",
    "user_reviews.drop(['latest_review_date', 'days_since_review', 'time_weight'], axis = 1, inplace = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 345,
   "metadata": {},
   "outputs": [],
   "source": [
    "# user_reviews.to_csv(user_reviews_target_csv, index = False)\n",
    "# item_metadata.to_csv(item_metadata_target_csv, index = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_reviews_grouped = user_reviews.groupby('parent_asin')['cleaned_review'].apply(lambda x: ' '.join(x)).reset_index()\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import torch\n",
    "\n",
    "# Load SBERT model\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "model = SentenceTransformer(\"all-MiniLM-L6-v2\").to(device)  # Fast & good for semantic similarity\n",
    "\n",
    "# Function to encode aggregated reviews\n",
    "def encode_reviews(text):\n",
    "    return model.encode(text, convert_to_numpy=True)\n",
    "\n",
    "# Generate embeddings for all products\n",
    "df_reviews_grouped['review_sbert_embedding'] = df_reviews_grouped['cleaned_review'].apply(encode_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 347,
   "metadata": {},
   "outputs": [],
   "source": [
    "reviews_compiled = pd.merge(left = df_product_sentiment, right = df_reviews_grouped.drop(['cleaned_review'], axis = 1), on = 'parent_asin', how = 'inner')\n",
    "item_metadata = pd.merge(left = item_metadata, right = reviews_compiled, on = 'parent_asin', how = 'left')\n",
    "item_metadata['product_sbert_embedding'] = item_metadata['cleaned_context'].apply(encode_reviews)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import MinMaxScaler\n",
    "\n",
    "def min_max_scale_dataframe(df, columns = []):\n",
    "    columns = df.select_dtypes(include=['number']).columns\n",
    "    scaler = MinMaxScaler()\n",
    "    df[columns] = scaler.fit_transform(df[columns])\n",
    "    \n",
    "    return df, scaler  # Return scaled DataFrame and scaler object\n",
    "\n",
    "item_metadata, scaler = min_max_scale_dataframe(item_metadata, columns=['average_rating', 'rating_number', 'no_of_reviews', 'rating_trend_slope', 'time_weighted_sentiment'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "# vectorizer = TfidfVectorizer(max_features=500)\n",
    "# tfidf_matrix = vectorizer.fit_transform(item_metadata['cleaned_context'].tolist())\n",
    "\n",
    "# # Convert to dense format\n",
    "# tfidf_dense = tfidf_matrix.toarray()\n",
    "\n",
    "# print(tfidf_dense.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def merge_features(df): # tfidf_matrix\n",
    "    num_cols = df.select_dtypes(include=['number']).columns\n",
    "    numerical_features = df[num_cols].to_numpy()\n",
    "    review_sbert_embeddings = np.array(df['review_sbert_embedding'].tolist())\n",
    "    product_sbert_embedding = np.array(df['product_sbert_embedding'].tolist())\n",
    "\n",
    "    # if not isinstance(tfidf_matrix, np.ndarray):\n",
    "    #     raise ValueError(\"TF-IDF matrix must be a NumPy array.\")\n",
    "\n",
    "    final_features = np.hstack((numerical_features, product_sbert_embedding, review_sbert_embeddings))\n",
    "\n",
    "    return final_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_features = item_metadata[['average_rating', 'rating_number', 'no_of_reviews', 'rating_trend_slope', 'time_weighted_sentiment', 'product_sbert_embedding', 'review_sbert_embedding']]\n",
    "feature_vector = merge_features(selected_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(7714, 773)"
      ]
     },
     "execution_count": 352,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "feature_vector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video_Games || Number of products = 7714\n",
      "Number of reviews = 383198\n"
     ]
    }
   ],
   "source": [
    "print(f'{category} || Number of products = {item_metadata.shape[0]}\\nNumber of reviews = {user_reviews.shape[0]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {},
   "outputs": [],
   "source": [
    "item_metadata.to_csv(f'Amazon Processed Dataset/{category}_item_metadata_w_features.csv', index = False)\n",
    "np.save(f'Amazon Processed Dataset/{category}_feature_vector.npy', feature_vector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 355,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import faiss\n",
    "\n",
    "# # Initialize FAISS index for L2 similarity\n",
    "# feature_dim = feature_vector.shape[1]\n",
    "# index = faiss.IndexFlatL2(feature_dim)\n",
    "\n",
    "# # Normalize feature vectors before adding (for cosine similarity)\n",
    "# final_features = feature_vector / np.linalg.norm(feature_vector, axis=1, keepdims=True)\n",
    "\n",
    "# # Add product vectors to FAISS\n",
    "# index.add(final_features)\n",
    "\n",
    "# print(f\"FAISS index contains {index.ntotal} items.\")\n",
    "# faiss.write_index(index, f\"Content-based Search Index/{category}_index.faiss\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 356,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def find_similar_products(product_id, index, feature_matrix, df, top_n=5):\n",
    "#     \"\"\"\n",
    "#     Finds the top-N most similar products to a given product ID.\n",
    "\n",
    "#     Parameters:\n",
    "#     - product_id (int): The index of the product in the DataFrame.\n",
    "#     - index (faiss.Index): FAISS index containing all product embeddings.\n",
    "#     - feature_matrix (np.ndarray): The full product feature matrix.\n",
    "#     - df (pd.DataFrame): Original DataFrame containing product info.\n",
    "#     - top_n (int): Number of similar products to return.\n",
    "\n",
    "#     Returns:\n",
    "#     - List of similar product details.\n",
    "#     \"\"\"\n",
    "#     # Query vector for the given product\n",
    "#     query_vector = feature_matrix[product_id].reshape(1, -1)\n",
    "\n",
    "#     # Search FAISS index for top-N similar items\n",
    "#     distances, indices = index.search(query_vector, top_n + 1)  # +1 to exclude self\n",
    "\n",
    "#     # Exclude the first result if it‚Äôs the query itself\n",
    "#     similar_indices = indices[0][1:] if indices[0][0] == product_id else indices[0][:top_n]\n",
    "\n",
    "#     # Fetch similar products\n",
    "#     similar_products = df.iloc[similar_indices]\n",
    "    \n",
    "#     return similar_products\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 357,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Example: Find similar products for product at index 0\n",
    "# product_id = 87127\n",
    "# print(f'Searching products similar to {item_metadata.loc[product_id, 'title']}...')\n",
    "# top_similar_products = find_similar_products(product_id, index, final_features, item_metadata, top_n=5)\n",
    "\n",
    "# # Print similar products\n",
    "# print(\"Top 5 similar products:\")\n",
    "# top_similar_products  # Adjust columns as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
